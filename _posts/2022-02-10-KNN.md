---
layout: post
title: KNN
subheading: Introduction to the KNN Algorithm.
author: Dongfang Zhao
categories: algorithm
banner:
  video: https://vjs.zencdn.net/v/oceans.mp4
  loop: true
  volume: 0.8
  start_at: 8.5
  image: https://bit.ly/3xTmdUP
  opacity: 0.618
  background: "#000"
  height: "100vh"
  min_height: "38vh"
  heading_style: "font-size: 4.25em; font-weight: bold; text-decoration: underline"
  subheading_style: "color: gold"
tags: KNN
sidebar: []
---

# KNN(K近邻算法)

全称：K-nearest neighbors algorithm(K近邻算法) 
所属分类：监督学习 
特点：非参数的(non-parametric algoriethm): 该算法不对基础数据做出任何假设。
			惰性学习的(lazy learning)：这种学习方法在最开始的时候不会根据已有的样本创建目标函数,只是简单的把训练用的样本储存好,																后期需要对新进入的样本进行判断的时候才开始分析新进入样本与已存在的训练样本之间的关系,并																据此确定新实例(新进入样本)的目标函数值.

算法概述：KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。简单说如果有A、B两类样本，这时有一个新的数据，计算距离该数据最近的K个样本所属的种类，进而判断出新数据的分类。一般计算距离的方式有曼哈顿距离，欧几里得距离。



The K-NN working can be explained on the basis of the below algorithm:

- **Step-1:** 选取一个K值，该值一般选择奇数，最少为3，偶数容易出现两边相同的情况。
- **Step-2:** 计算新数据点和所有点之间的距离
- **Step-3:** 从所有计算出的距离中选择K个离新数据最近的
- **Step-4:** 从这K个数据中计算他们所属的分类
- **Step-5:** 将新的数据点归类为K个数据中所属分类的点最多的那个类中。
- **Step-6:** 结束

### KNN算法的优势：

-实施起来很简单。
-它对噪声训练数据具有鲁棒性
-如果训练数据很大，它可以更有效。

### KNN算法的缺点：

-总是需要确定K的值，K的值可能在某个时候很复杂。
-计算成本高，因为计算所有训练样本的数据点之间的距离。
-惰性学习，每次数据都要重新计算，慢！！！！！！！！

简单实现：

```python
from sklearn.neighbors import KNeighborsClassifier
# 算法器
knn = KNeighborsClassifier(n_neighbors=k, p=2, metric="minkowski")
# 拟合数据
classifier.fit(features, pred)
# 预测
ret = knn.predict()
```
